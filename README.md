# DetoxiPy

This project attempts to classify toxicity in online comments, while also reducing the “unintended bias” that gets introduced into the models.

[Description of Data](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data)

The top level directory contains the "src" folder which contains the core code for the project, and the "EDA" folder which contains the notebooks performing the EDA on the text data. 

Please refer the readme within src for full details on project structure and modules. 

### References

Jigsaw. “Unintended Bias in Toxicity Classification”. Kaggle, 2019

Jigsaw. “Unintended Bias and Names of Frequently Targeted Groups.” Medium, 2018.

[Understanding LSTM Networks (Colah’s Blog, 2015)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[Kaggle](https://www.kaggle.com/bminixhofer/simple-lstm-pytorch-version)

[LSTM in PyTorch (Holzner)](https://medium.com/@andre.holzner/lstm-cells-in-pytorch-fab924a78b1c)

