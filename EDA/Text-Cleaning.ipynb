{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib.request\n",
    "import urlopen\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import operator \n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "from edm import report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Text Cleaning and Embedding Initialization\n",
    "> May 5, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) # disable sci notation\n",
    "font = {'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global params\n",
    "SPL_SEQ_DICT = {\"emojis\": [\":)\", \":-)\", \":(\", \":-(\", \":-/\", \":/\", \"-_-\", \":|\",  \":-|\"],\n",
    "                \"proper nouns\": [\"republican\", \"democrat\", \"trump\", \"clinton\", \"hillary\"]}\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \",\n",
    "                 \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\",\n",
    "                 '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞':'infinity', 'θ': 'theta',\n",
    "                 '÷': '/', 'α': 'alpha','•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '',\n",
    "            '³': '3', 'π': 'pi', }\n",
    "\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "\n",
    "mispell_dict = {'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','Gʀᴇat':'great','ʙᴏᴛtoᴍ':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yᴏᴜ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','ᴀ':'a', '😉':'wink','😂':'joy','😀':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}\n",
    "\n",
    "# ref: https://www.kaggle.com/adityaecdrid/public-version-text-cleaning-vocab-65\n",
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because',',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not',\n",
    "    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n",
    "    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n",
    "    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n",
    "    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n",
    "    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n",
    "    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n",
    "    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n",
    "    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n",
    "    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n",
    "    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n",
    "    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n",
    "    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n",
    "    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n",
    "    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n",
    "    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n",
    "    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n",
    "    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n",
    "    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n",
    "    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n",
    "    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n",
    "    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n",
    "    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I',\n",
    "    'ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer',\\\n",
    "    'ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first',\\\n",
    "    'ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra',\\\n",
    "    'Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\\\n",
    "    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\\\n",
    "    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n",
    "    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "train = pd.read_csv(\"/Users/elenabg/DetoxiPy/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new columns\n",
    "\n",
    "train['total_length'] = train['comment_text'].apply(len)\n",
    "train['capitals'] = train['comment_text'].apply(lambda comment: \\\n",
    "                               sum(1 for c in comment if c.isupper()))\n",
    "train['caps_ratio'] = train.apply(lambda row: \\\n",
    "                      float(row['capitals'])/float(row['total_length']),axis=1)\n",
    "train['num_exclamation_marks'] = train['comment_text'].apply(lambda \\\n",
    "                                                             comment: comment.count('!'))\n",
    "train['excl_ratio'] = train.apply(lambda row: \\\n",
    "                      float(row['num_exclamation_marks'])/float(row['total_length']),axis=1)\n",
    "train['num_question_marks'] = train['comment_text'].apply(lambda comment: comment.count('?'))\n",
    "\n",
    "train['quest_ratio'] = train.apply(lambda row: \\\n",
    "                      float(row['num_question_marks'])/float(row['total_length']),axis=1)\n",
    "train['num_punctuation'] = train['comment_text'].apply(lambda \\\n",
    "                                            comment: sum(comment.count(w) for w in '.,;:'))\n",
    "\n",
    "train['punct_ratio'] = train.apply(lambda row: \\\n",
    "                      float(row['num_punctuation'])/float(row['total_length']),axis=1)\n",
    "train['num_symbols'] = train['comment_text'].apply(lambda comment:\\\n",
    "                sum(comment.count(w) for w in '*&$%'))\n",
    "train['symb_ratio'] = train.apply(lambda row: \\\n",
    "                      float(row['num_symbols'])/float(row['total_length']),axis=1)\n",
    "train['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))\n",
    "train['num_unique_words'] = train['comment_text'].apply(lambda comment: len(set(w for \\\n",
    "                                                        w in comment.split())))\n",
    "train['unique_ratio'] = train['num_unique_words'] / train['num_words']\n",
    "train['num_smilies'] = train['comment_text'].apply(lambda comment: \\\n",
    "                           sum(comment.count(w) for w in SPL_SEQ_DICT[\"emojis\"] ))\n",
    "train['prop_nouns_num'] = train['comment_text'].apply(lambda comment: \\\n",
    "               sum(comment.lower().count(w) for w in SPL_SEQ_DICT[\"proper nouns\"] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "def add_lower(embedding, vocab):\n",
    "    '''Since we are not going to get rid of uppercase, \n",
    "    add lowercase versions of words to embedding'''\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")\n",
    "\n",
    "\n",
    "def check_unknown_punct(embed, punct):\n",
    "    '''Similar to lowercase, add punctuation to \n",
    "    embeddings'''\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''\n",
    "    Very simple correction for commonly mispelled words/terms in the text\n",
    "    '''\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed coverage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    if file == '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec':\n",
    "        embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(crawl)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "    return embeddings_index\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    '''Check how many words are known in \n",
    "    the built-in glove embedding'''\n",
    "    \n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check embeddings between cleaning stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove embeddings from file (can be dowloaded here: https://www.kaggle.com/takuok/glove840b300dtxt#glove.840B.300d.txt)\n",
    "\n",
    "file = \"/Users/elenabg/Documents/6Q/AML/Project/glove.840B.300d.txt\"\n",
    "embed_glove = load_embed(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embedding\n",
    "pickle.dump(embed_glove, open(\"/Users/elenabg/Documents/6Q/AML/Project/glvembed.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Initial vocabulary from raw text\n",
    "vocab = build_vocab(train['comment_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 15.76% of vocab\n",
      "Found embeddings for  89.60% of all text\n"
     ]
    }
   ],
   "source": [
    "#1. Check initial coverage\n",
    "\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Added 24243 words to embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"isn't\", 39964),\n",
       " (\"That's\", 37640),\n",
       " (\"won't\", 29397),\n",
       " (\"he's\", 24353),\n",
       " (\"Trump's\", 23453),\n",
       " (\"aren't\", 20528),\n",
       " (\"wouldn't\", 19544),\n",
       " ('Yes,', 19043),\n",
       " ('that,', 18283),\n",
       " (\"wasn't\", 18153)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Add (missing) lower-case to embedding\n",
    "\n",
    "print(\"Glove : \")\n",
    "add_lower(embed_glove, vocab)\n",
    "\n",
    "# Check Result\n",
    "oov_glove[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove (Unkown punc %):\n",
      "“ ” ’ ∞ θ ÷ α • à − β ∅ ³ π ‘ ₹ ´ ° £ € × ™ √ ² — – \n"
     ]
    }
   ],
   "source": [
    "# 3. Punctuation\n",
    "\n",
    "print(\"Glove (Unkown punc %):\")\n",
    "print(check_unknown_punct(embed_glove, punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 62.64% of vocab\n",
      "Found embeddings for  99.72% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tRump', 2525),\n",
       " ('Brexit', 1740),\n",
       " ('theglobeandmail', 1351),\n",
       " ('Québec', 1331),\n",
       " ('Drumpf', 1186),\n",
       " ('deplorables', 1026),\n",
       " ('SB91', 781),\n",
       " ('theguardian', 735),\n",
       " ('Trumpcare', 570),\n",
       " ('✰', 550)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['treated_comment'] = train['comment_text'].apply(lambda x: clean_special_chars(x,\n",
    "                                                                    punct, punct_mapping))\n",
    "new_vocab = build_vocab(train['treated_comment'])\n",
    "\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(new_vocab, embed_glove)\n",
    "oov_glove[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Known Contractions -\n",
      "   Glove :\n",
      "[\"'cause\", \"can't\", \"didn't\", \"doesn't\", \"don't\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"It's\", \"it's\", \"ma'am\", \"that's\", \"you'll\", \"you're\", 'you.i', \"c'mon\", \"d'int\"]\n"
     ]
    }
   ],
   "source": [
    "# 4 Contractions\n",
    "\n",
    "print(\"- Known Contractions -\")\n",
    "print(\"   Glove :\")\n",
    "print(known_contractions(embed_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 62.65% of vocab\n",
      "Found embeddings for  99.72% of all text\n"
     ]
    }
   ],
   "source": [
    "train['treated_comment'] = train['treated_comment'].apply(lambda x: clean_contractions(x,\n",
    "                                                        contraction_mapping))\n",
    "new_vocab = build_vocab(train['treated_comment'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(new_vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 62.66% of vocab\n",
      "Found embeddings for  99.74% of all text\n"
     ]
    }
   ],
   "source": [
    "# 5. Fixed spelling\n",
    "train['treated_comment'] = train['treated_comment'].apply(lambda x: correct_spelling(x,\n",
    "                                        mispell_dict))\n",
    "new_vocab = build_vocab(train['treated_comment'])\n",
    "print(\"Glove : \")\n",
    "oov_glove = check_coverage(new_vocab, embed_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure \"difficulty\" of initial/final training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.sample(frac=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Building bag of words representations...\n",
      "[-------------------------------] : 5414 of 5415, 100.0% : Est. 0.0 mins Remaining               ] : 154 of 5415, 2.8% : Est. 0.3 mins Remaining    ] : 222 of 5415, 4.1% : Est. 0.3 mins Remaining] : 294 of 5415, 5.4% : Est. 0.3 mins Remaining                         ] : 372 of 5415, 6.9% : Est. 0.2 mins Remaining             ] : 435 of 5415, 8.0% : Est. 0.3 mins Remaining                    ] : 492 of 5415, 9.1% : Est. 0.3 mins Remaining                          ] : 576 of 5415, 10.6% : Est. 0.2 mins Remaining                      ] : 654 of 5415, 12.1% : Est. 0.2 mins Remaining     ] : 730 of 5415, 13.5% : Est. 0.2 mins Remaining   ] : 794 of 5415, 14.7% : Est. 0.2 mins Remaining-----                         ] : 854 of 5415, 15.8% : Est. 0.2 mins Remaining                ] : 935 of 5415, 17.3% : Est. 0.2 mins Remaining             ] : 1013 of 5415, 18.7% : Est. 0.2 mins Remaining             ] : 1083 of 5415, 20.0% : Est. 0.2 mins Remaining---                       ] : 1154 of 5415, 21.3% : Est. 0.2 mins Remaining                      ] : 1232 of 5415, 22.8% : Est. 0.2 mins Remaining                   ] : 1310 of 5415, 24.2% : Est. 0.2 mins Remaining   ] : 1372 of 5415, 25.3% : Est. 0.2 mins Remaining                    ] : 1445 of 5415, 26.7% : Est. 0.2 mins Remaining                ] : 1513 of 5415, 27.9% : Est. 0.2 mins Remaining-                     ] : 1591 of 5415, 29.4% : Est. 0.2 mins Remaining---------                    ] : 1654 of 5415, 30.5% : Est. 0.2 mins Remaining------                    ] : 1732 of 5415, 32.0% : Est. 0.2 mins Remaining          ] : 1794 of 5415, 33.1% : Est. 0.2 mins Remaining       ] : 1872 of 5415, 34.6% : Est. 0.2 mins Remaining   ] : 1950 of 5415, 36.0% : Est. 0.2 mins Remaining] : 2028 of 5415, 37.5% : Est. 0.2 mins Remaining----------                  ] : 2107 of 5415, 38.9% : Est. 0.2 mins Remaining                 ] : 2192 of 5415, 40.5% : Est. 0.2 mins Remaining----                 ] : 2263 of 5415, 41.8% : Est. 0.2 mins Remaining--                ] : 2341 of 5415, 43.2% : Est. 0.2 mins Remaining              ] : 2419 of 5415, 44.7% : Est. 0.1 mins Remaining           ] : 2497 of 5415, 46.1% : Est. 0.1 mins Remaining------------               ] : 2563 of 5415, 47.3% : Est. 0.1 mins Remaining        ] : 2700 of 5415, 49.9% : Est. 0.1 mins Remaining------------              ] : 2766 of 5415, 51.1% : Est. 0.1 mins Remaining         ] : 2821 of 5415, 52.1% : Est. 0.1 mins Remaining             ] : 2886 of 5415, 53.3% : Est. 0.1 mins Remaining----------             ] : 2961 of 5415, 54.7% : Est. 0.1 mins Remaining-----------------             ] : 3023 of 5415, 55.8% : Est. 0.1 mins Remaining   ] : 3106 of 5415, 57.4% : Est. 0.1 mins Remaining------------------            ] : 3183 of 5415, 58.8% : Est. 0.1 mins Remaining-----           ] : 3247 of 5415, 60.0% : Est. 0.1 mins Remaining--           ] : 3325 of 5415, 61.4% : Est. 0.1 mins Remaining------------------           ] : 3388 of 5415, 62.6% : Est. 0.1 mins Remaining       ] : 3452 of 5415, 63.7% : Est. 0.1 mins Remaining          ] : 3528 of 5415, 65.2% : Est. 0.1 mins Remaining       ] : 3606 of 5415, 66.6% : Est. 0.1 mins Remaining--------------         ] : 3669 of 5415, 67.8% : Est. 0.1 mins Remaining-----------         ] : 3747 of 5415, 69.2% : Est. 0.1 mins Remaining--------        ] : 3825 of 5415, 70.6% : Est. 0.1 mins Remaining-----        ] : 3903 of 5415, 72.1% : Est. 0.1 mins Remaining--       ] : 3981 of 5415, 73.5% : Est. 0.1 mins Remaining      ] : 4059 of 5415, 75.0% : Est. 0.1 mins Remaining-------       ] : 4137 of 5415, 76.4% : Est. 0.1 mins Remaining-------------      ] : 4193 of 5415, 77.4% : Est. 0.1 mins Remaining     ] : 4267 of 5415, 78.8% : Est. 0.1 mins Remaining    ] : 4345 of 5415, 80.2% : Est. 0.1 mins Remaining-----------------------     ] : 4419 of 5415, 81.6% : Est. 0.0 mins Remaining--------------------     ] : 4497 of 5415, 83.0% : Est. 0.0 mins Remaining-----------------    ] : 4575 of 5415, 84.5% : Est. 0.0 mins Remaining--------------    ] : 4653 of 5415, 85.9% : Est. 0.0 mins Remaining-----------   ] : 4731 of 5415, 87.4% : Est. 0.0 mins Remaining--------   ] : 4809 of 5415, 88.8% : Est. 0.0 mins Remaining-----  ] : 4887 of 5415, 90.2% : Est. 0.0 mins Remaining-----------  ] : 4964 of 5415, 91.7% : Est. 0.0 mins Remaining------------------  ] : 5028 of 5415, 92.9% : Est. 0.0 mins Remaining---------------- ] : 5106 of 5415, 94.3% : Est. 0.0 mins Remaining------------------- ] : 5167 of 5415, 95.4% : Est. 0.0 mins Remaining------------------------------] : 5247 of 5415, 96.9% : Est. 0.0 mins Remaining--------------------------] : 5325 of 5415, 98.3% : Est. 0.0 mins Remaining--------------------------] : 5403 of 5415, 99.8% : Est. 0.0 mins Remaining\n",
      "----> Done.\n",
      "----> Getting difficulty metrics...\n",
      "----> Done.\n",
      "----> Getting generic statistics...\n",
      "----> Done.\n",
      "\n",
      "\n",
      "Dataset Size                 5415                -\n",
      "Vocab Size                   22215               -\n",
      "Number of Classes            198                 -\n",
      "Mean Items Per Class         27.348484848484848  -\n",
      "Min. Items in a Class        1                   \u001b[91mEXTREMELY LOW\u001b[0m\n",
      "Average Sentence Length      300.22622345337027  -\n",
      "Distinct Words : Total Words 0.07975572453309782 \u001b[92mGOOD\u001b[0m\n",
      "Class Imbalance              1.8093679174011585  \u001b[91mVERY HIGH\u001b[0m\n",
      "Class Diversity              1.4126595289671613  \u001b[94mSOMEWHAT HIGH\u001b[0m\n",
      "Max. Hellinger Similarity    0.6973940715992522  \u001b[94mSOMEWHAT HIGH\u001b[0m\n",
      "Mutual Information           0.04234369966082274 \u001b[92mGOOD\u001b[0m\n",
      "Difficulty                   4.041520942161492   \u001b[94mSOMEWHAT HIGH\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial texts\n",
    "sents = df[\"comment_text\"].values\n",
    "labels = df[\"target\"].values\n",
    "print(report.get_difficulty_report(sents, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> Building bag of words representations...\n",
      "[-------------------------------] : 5414 of 5415, 100.0% : Est. 0.0 mins Remaining                       ] : 87 of 5415, 1.6% : Est. 0.6 mins Remaining                    ] : 147 of 5415, 2.7% : Est. 0.5 mins Remaining-                            ] : 212 of 5415, 3.9% : Est. 0.4 mins Remaining ] : 284 of 5415, 5.2% : Est. 0.4 mins Remaining              ] : 343 of 5415, 6.3% : Est. 0.3 mins Remaining    ] : 414 of 5415, 7.6% : Est. 0.3 mins Remaining           ] : 477 of 5415, 8.8% : Est. 0.3 mins Remaining                        ] : 618 of 5415, 11.4% : Est. 0.3 mins Remaining                     ] : 696 of 5415, 12.9% : Est. 0.3 mins Remaining      ] : 769 of 5415, 14.2% : Est. 0.3 mins Remaining  ] : 836 of 5415, 15.4% : Est. 0.3 mins Remaining------                        ] : 915 of 5415, 16.9% : Est. 0.3 mins Remaining---                        ] : 993 of 5415, 18.3% : Est. 0.2 mins Remaining                       ] : 1071 of 5415, 19.8% : Est. 0.2 mins Remaining                    ] : 1149 of 5415, 21.2% : Est. 0.2 mins Remaining    ] : 1211 of 5415, 22.4% : Est. 0.2 mins Remaining ] : 1289 of 5415, 23.8% : Est. 0.2 mins Remaining-------                      ] : 1368 of 5415, 25.3% : Est. 0.2 mins Remaining-----                     ] : 1446 of 5415, 26.7% : Est. 0.2 mins Remaining-                     ] : 1524 of 5415, 28.1% : Est. 0.2 mins Remaining                    ] : 1595 of 5415, 29.5% : Est. 0.2 mins Remaining               ] : 1680 of 5415, 31.0% : Est. 0.2 mins Remaining            ] : 1758 of 5415, 32.5% : Est. 0.2 mins Remaining        ] : 1836 of 5415, 33.9% : Est. 0.2 mins Remaining     ] : 1914 of 5415, 35.3% : Est. 0.2 mins Remaining ] : 1992 of 5415, 36.8% : Est. 0.2 mins Remaining------------                  ] : 2071 of 5415, 38.2% : Est. 0.2 mins Remaining   ] : 2144 of 5415, 39.6% : Est. 0.2 mins Remaining           ] : 2211 of 5415, 40.8% : Est. 0.2 mins Remaining       ] : 2289 of 5415, 42.3% : Est. 0.2 mins Remaining--------                ] : 2352 of 5415, 43.4% : Est. 0.2 mins Remaining----                ] : 2430 of 5415, 44.9% : Est. 0.2 mins Remaining-                ] : 2508 of 5415, 46.3% : Est. 0.2 mins Remaining             ] : 2586 of 5415, 47.8% : Est. 0.1 mins Remaining          ] : 2664 of 5415, 49.2% : Est. 0.1 mins Remaining      ] : 2742 of 5415, 50.6% : Est. 0.1 mins Remaining---------------             ] : 2893 of 5415, 53.4% : Est. 0.1 mins Remaining            ] : 2947 of 5415, 54.4% : Est. 0.1 mins Remaining-----------------             ] : 3007 of 5415, 55.5% : Est. 0.1 mins Remaining     ] : 3066 of 5415, 56.6% : Est. 0.1 mins Remaining------------------            ] : 3133 of 5415, 57.9% : Est. 0.1 mins Remaining-----------------            ] : 3192 of 5415, 58.9% : Est. 0.1 mins Remaining      ] : 3246 of 5415, 59.9% : Est. 0.1 mins Remaining     ] : 3311 of 5415, 61.1% : Est. 0.1 mins Remaining-------           ] : 3380 of 5415, 62.4% : Est. 0.1 mins Remaining--------------------          ] : 3435 of 5415, 63.4% : Est. 0.1 mins Remaining--------          ] : 3496 of 5415, 64.6% : Est. 0.1 mins Remaining-------          ] : 3539 of 5415, 65.4% : Est. 0.1 mins Remaining-----------          ] : 3586 of 5415, 66.2% : Est. 0.1 mins Remaining      ] : 3648 of 5415, 67.4% : Est. 0.1 mins Remaining-------------         ] : 3711 of 5415, 68.5% : Est. 0.1 mins Remaining-----------        ] : 3789 of 5415, 70.0% : Est. 0.1 mins Remaining-------        ] : 3867 of 5415, 71.4% : Est. 0.1 mins Remaining----        ] : 3945 of 5415, 72.9% : Est. 0.1 mins Remaining-       ] : 4023 of 5415, 74.3% : Est. 0.1 mins Remaining     ] : 4101 of 5415, 75.7% : Est. 0.1 mins Remaining-----      ] : 4175 of 5415, 77.1% : Est. 0.1 mins Remaining------------      ] : 4242 of 5415, 78.3% : Est. 0.1 mins Remaining--------      ] : 4320 of 5415, 79.8% : Est. 0.1 mins Remaining------     ] : 4398 of 5415, 81.2% : Est. 0.1 mins Remaining-     ] : 4470 of 5415, 82.5% : Est. 0.1 mins Remaining--------------------    ] : 4539 of 5415, 83.8% : Est. 0.0 mins Remaining----------------    ] : 4617 of 5415, 85.3% : Est. 0.0 mins Remaining--------------   ] : 4695 of 5415, 86.7% : Est. 0.0 mins Remaining----------   ] : 4773 of 5415, 88.1% : Est. 0.0 mins Remaining-------   ] : 4851 of 5415, 89.6% : Est. 0.0 mins Remaining----------------------------  ] : 4918 of 5415, 90.8% : Est. 0.0 mins Remaining-------------------------  ] : 5039 of 5415, 93.1% : Est. 0.0 mins Remaining----------- ] : 5101 of 5415, 94.2% : Est. 0.0 mins Remaining------- ] : 5179 of 5415, 95.6% : Est. 0.0 mins Remaining-------------------------] : 5242 of 5415, 96.8% : Est. 0.0 mins Remaining---------------------] : 5320 of 5415, 98.2% : Est. 0.0 mins Remaining------------------] : 5398 of 5415, 99.7% : Est. 0.0 mins Remaining\n",
      "----> Done.\n",
      "----> Getting difficulty metrics...\n",
      "----> Done.\n",
      "----> Getting generic statistics...\n",
      "----> Done.\n",
      "\n",
      "\n",
      "Dataset Size                 5415                -\n",
      "Vocab Size                   20505               -\n",
      "Number of Classes            198                 -\n",
      "Mean Items Per Class         27.348484848484848  -\n",
      "Min. Items in a Class        1                   \u001b[91mEXTREMELY LOW\u001b[0m\n",
      "Average Sentence Length      325.63416435826406  -\n",
      "Distinct Words : Total Words 0.07101323294626129 \u001b[92mGOOD\u001b[0m\n",
      "Class Imbalance              1.8093679174011585  \u001b[91mVERY HIGH\u001b[0m\n",
      "Class Diversity              1.4126595289671613  \u001b[94mSOMEWHAT HIGH\u001b[0m\n",
      "Max. Hellinger Similarity    0.7066384880987086  \u001b[94mSOMEWHAT HIGH\u001b[0m\n",
      "Mutual Information           0.04369636289887571 \u001b[92mGOOD\u001b[0m\n",
      "Difficulty                   4.043375530312165   \u001b[94mSOMEWHAT HIGH\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean text difficulty\n",
    "sents_fin = df[\"treated_comment\"].values\n",
    "print(report.get_difficulty_report(sents_fin, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clean training set\n",
    "pickle.dump(train, open(\"/Users/elenabg/Documents/6Q/AML/Project/clean_train.p\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
